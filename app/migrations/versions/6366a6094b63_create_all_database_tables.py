"""Create all database tables

Revision ID: 6366a6094b63
Revises:
Create Date: 2025-11-09 10:34:50.630104

"""

from typing import Sequence, Union

import sqlalchemy as sa
from alembic import op
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = "6366a6094b63"
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "tickers",
        sa.Column("ticker_id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("symbol", sa.String(length=20), nullable=False),
        sa.Column("name", sa.String(length=255), nullable=True),
        sa.Column("exchange", sa.String(length=50), nullable=True),
        sa.Column("currency", sa.String(length=10), nullable=True),
        sa.Column("sector", sa.String(length=100), nullable=True),
        sa.Column("industry", sa.String(length=100), nullable=True),
        sa.Column("beta", sa.Numeric(precision=6, scale=3), nullable=True),
        sa.Column(
            "fifty_two_week_high", sa.Numeric(precision=10, scale=2), nullable=True
        ),
        sa.Column(
            "fifty_two_week_low", sa.Numeric(precision=10, scale=2), nullable=True
        ),
        sa.Column("is_active", sa.Boolean(), nullable=False),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.PrimaryKeyConstraint("ticker_id"),
    )
    op.create_index(op.f("ix_tickers_industry"), "tickers", ["industry"], unique=False)
    op.create_index(op.f("ix_tickers_sector"), "tickers", ["sector"], unique=False)
    op.create_index(op.f("ix_tickers_symbol"), "tickers", ["symbol"], unique=True)
    op.create_table(
        "watchlists",
        sa.Column("watchlist_id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("name", sa.String(length=100), nullable=False),
        sa.Column("description", sa.Text(), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.PrimaryKeyConstraint("watchlist_id"),
        sa.UniqueConstraint("name"),
    )
    op.create_table(
        "analyst_ratings",
        sa.Column("rating_id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("ticker_id", sa.Integer(), nullable=False),
        sa.Column("rating_date", sa.Date(), nullable=False),
        sa.Column(
            "rating",
            sa.Enum(
                "strong_buy", "buy", "hold", "sell", "strong_sell", name="rating_enum"
            ),
            nullable=False,
        ),
        sa.Column("firm", sa.String(length=100), nullable=True),
        sa.Column(
            "action",
            sa.Enum(
                "upgrade",
                "downgrade",
                "maintain",
                "initiate",
                name="rating_action_enum",
            ),
            nullable=True,
        ),
        sa.Column("target_price", sa.Numeric(precision=10, scale=2), nullable=True),
        sa.Column("retrieved_at", sa.DateTime(timezone=True), nullable=False),
        sa.ForeignKeyConstraint(
            ["ticker_id"], ["tickers.ticker_id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("rating_id"),
    )
    op.create_index(
        "idx_rating_ticker_date",
        "analyst_ratings",
        ["ticker_id", "rating_date"],
        unique=False,
        postgresql_using="btree",
        postgresql_ops={"rating_date": "DESC"},
    )
    op.create_index(
        op.f("ix_analyst_ratings_rating_date"),
        "analyst_ratings",
        ["rating_date"],
        unique=False,
    )
    op.create_index(
        "uq_rating_ticker_date_firm",
        "analyst_ratings",
        ["ticker_id", "rating_date", "firm"],
        unique=True,
        postgresql_where="firm IS NOT NULL",
    )
    op.create_table(
        "collection_jobs",
        sa.Column("job_id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("ticker_id", sa.Integer(), nullable=True),
        sa.Column("job_type", sa.String(length=20), nullable=False),
        sa.Column("start_date", sa.Date(), nullable=True),
        sa.Column("end_date", sa.Date(), nullable=True),
        sa.Column("status", sa.String(length=20), nullable=False),
        sa.Column(
            "started_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=True,
        ),
        sa.Column("completed_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("records_fetched", sa.Integer(), nullable=True),
        sa.Column("records_inserted", sa.Integer(), nullable=True),
        sa.Column("records_updated", sa.Integer(), nullable=True),
        sa.Column("error_message", sa.Text(), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.CheckConstraint(
            "job_type IN ('single', 'batch', 'scheduled')", name="chk_job_type_valid"
        ),
        sa.CheckConstraint(
            "status IN ('pending', 'running', 'completed', 'failed', 'partial')",
            name="chk_status_valid",
        ),
        sa.ForeignKeyConstraint(
            ["ticker_id"], ["tickers.ticker_id"], ondelete="SET NULL"
        ),
        sa.PrimaryKeyConstraint("job_id"),
    )
    op.create_index(
        "idx_collection_jobs_started", "collection_jobs", ["started_at"], unique=False
    )
    op.create_index(
        op.f("ix_collection_jobs_status"), "collection_jobs", ["status"], unique=False
    )
    op.create_index(
        op.f("ix_collection_jobs_ticker_id"),
        "collection_jobs",
        ["ticker_id"],
        unique=False,
    )
    op.create_table(
        "collection_schedules",
        sa.Column("schedule_id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("watchlist_id", sa.Integer(), nullable=False),
        sa.Column("frequency", sa.String(length=20), nullable=False),
        sa.Column("execution_time", sa.Time(), nullable=False),
        sa.Column("is_enabled", sa.Boolean(), nullable=False),
        sa.Column(
            "data_types",
            postgresql.JSON(astext_type=sa.Text()),
            server_default='["price"]',
            nullable=False,
        ),
        sa.Column("last_run_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("next_run_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.CheckConstraint(
            "frequency IN ('daily', 'weekly')", name="chk_frequency_valid"
        ),
        sa.ForeignKeyConstraint(
            ["watchlist_id"], ["watchlists.watchlist_id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("schedule_id"),
    )
    op.create_index(
        "idx_collection_schedules_next_run",
        "collection_schedules",
        ["next_run_at"],
        unique=False,
        postgresql_where=sa.text("is_enabled = true"),
    )
    op.create_table(
        "daily_prices",
        sa.Column("price_id", sa.BigInteger(), autoincrement=True, nullable=False),
        sa.Column("ticker_id", sa.Integer(), nullable=False),
        sa.Column("date", sa.Date(), nullable=False),
        sa.Column("open", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("high", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("low", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("close", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("adj_close", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("volume", sa.BigInteger(), nullable=True),
        sa.Column("sma_5", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("sma_25", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("sma_75", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("ema_12", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("ema_26", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("rsi_14", sa.Numeric(precision=6, scale=2), nullable=True),
        sa.Column("stoch_k", sa.Numeric(precision=6, scale=2), nullable=True),
        sa.Column("stoch_d", sa.Numeric(precision=6, scale=2), nullable=True),
        sa.Column("macd", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("macd_signal", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("macd_histogram", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("bb_upper", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("bb_middle", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("bb_lower", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("bb_width", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("atr_14", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column(
            "realized_volatility", sa.Numeric(precision=8, scale=4), nullable=True
        ),
        sa.Column("adx_14", sa.Numeric(precision=6, scale=2), nullable=True),
        sa.Column("sar", sa.Numeric(precision=12, scale=4), nullable=True),
        sa.Column("obv", sa.BigInteger(), nullable=True),
        sa.Column("volume_ma_20", sa.BigInteger(), nullable=True),
        sa.Column("volume_ratio", sa.Numeric(precision=8, scale=4), nullable=True),
        sa.Column(
            "data_quality_score", sa.Numeric(precision=3, scale=2), nullable=True
        ),
        sa.Column("is_repaired", sa.Boolean(), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.CheckConstraint(
            "open > 0 AND high >= low AND high >= open "
            "AND high >= close AND low <= open AND low <= close",
            name="chk_ohlc_valid",
        ),
        sa.ForeignKeyConstraint(
            ["ticker_id"], ["tickers.ticker_id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("price_id"),
        sa.UniqueConstraint("ticker_id", "date", name="uq_ticker_date"),
    )
    op.create_index("idx_daily_prices_date", "daily_prices", ["date"], unique=False)
    op.create_index(
        "idx_daily_prices_ticker_date",
        "daily_prices",
        ["ticker_id", "date"],
        unique=False,
        postgresql_using="btree",
    )
    op.create_table(
        "earnings_data",
        sa.Column("earnings_id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("ticker_id", sa.Integer(), nullable=False),
        sa.Column("fiscal_quarter", sa.String(length=10), nullable=False),
        sa.Column("fiscal_year", sa.Integer(), nullable=False),
        sa.Column("earnings_date", sa.Date(), nullable=True),
        sa.Column("reported_eps", sa.Numeric(precision=20, scale=4), nullable=True),
        sa.Column("estimated_eps", sa.Numeric(precision=20, scale=4), nullable=True),
        sa.Column("surprise_pct", sa.Numeric(precision=6, scale=2), nullable=True),
        sa.Column("revenue", sa.BigInteger(), nullable=True),
        sa.Column("retrieved_at", sa.DateTime(timezone=True), nullable=False),
        sa.ForeignKeyConstraint(
            ["ticker_id"], ["tickers.ticker_id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("earnings_id"),
    )
    op.create_index(
        "idx_earnings_ticker_quarter",
        "earnings_data",
        ["ticker_id", "fiscal_year", "fiscal_quarter"],
        unique=False,
        postgresql_using="btree",
        postgresql_ops={"fiscal_year": "DESC"},
    )
    op.create_index(
        "uq_earnings_ticker_period",
        "earnings_data",
        ["ticker_id", "fiscal_quarter", "fiscal_year"],
        unique=True,
    )
    op.create_table(
        "financial_statements",
        sa.Column("statement_id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("ticker_id", sa.Integer(), nullable=False),
        sa.Column(
            "statement_type",
            sa.Enum(
                "income_statement",
                "balance_sheet",
                "cash_flow",
                name="statement_type_enum",
            ),
            nullable=False,
        ),
        sa.Column("fiscal_quarter", sa.String(length=10), nullable=False),
        sa.Column("fiscal_year", sa.Integer(), nullable=False),
        sa.Column("line_item", sa.String(length=100), nullable=False),
        sa.Column("value", sa.BigInteger(), nullable=True),
        sa.Column("currency", sa.String(length=10), nullable=False),
        sa.Column("retrieved_at", sa.DateTime(timezone=True), nullable=False),
        sa.ForeignKeyConstraint(
            ["ticker_id"], ["tickers.ticker_id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("statement_id"),
    )
    op.create_index(
        "idx_statement_line_item", "financial_statements", ["line_item"], unique=False
    )
    op.create_index(
        "idx_statement_ticker_type_quarter",
        "financial_statements",
        ["ticker_id", "statement_type", "fiscal_year", "fiscal_quarter"],
        unique=False,
        postgresql_using="btree",
        postgresql_ops={"fiscal_year": "DESC"},
    )
    op.create_index(
        "uq_statement_ticker_line_item",
        "financial_statements",
        ["ticker_id", "statement_type", "fiscal_quarter", "fiscal_year", "line_item"],
        unique=True,
    )
    op.create_table(
        "fundamental_data",
        sa.Column("fundamental_id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("ticker_id", sa.Integer(), nullable=False),
        sa.Column("retrieved_at", sa.DateTime(timezone=True), nullable=False),
        sa.Column("eps_trailing", sa.Numeric(precision=20, scale=4), nullable=True),
        sa.Column("eps_forward", sa.Numeric(precision=20, scale=4), nullable=True),
        sa.Column("per_trailing", sa.Numeric(precision=10, scale=2), nullable=True),
        sa.Column("per_forward", sa.Numeric(precision=10, scale=2), nullable=True),
        sa.Column("peg_ratio", sa.Numeric(precision=10, scale=2), nullable=True),
        sa.Column("market_cap", sa.BigInteger(), nullable=True),
        sa.Column("dividend_yield", sa.Numeric(precision=5, scale=4), nullable=True),
        sa.Column("profit_margin", sa.Numeric(precision=5, scale=4), nullable=True),
        sa.Column("earnings_growth", sa.Numeric(precision=6, scale=4), nullable=True),
        sa.ForeignKeyConstraint(
            ["ticker_id"], ["tickers.ticker_id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("fundamental_id"),
    )
    op.create_index(
        "idx_fundamental_ticker_date",
        "fundamental_data",
        ["ticker_id", "retrieved_at"],
        unique=False,
        postgresql_using="btree",
        postgresql_ops={"retrieved_at": "DESC"},
    )
    op.create_index(
        op.f("ix_fundamental_data_retrieved_at"),
        "fundamental_data",
        ["retrieved_at"],
        unique=False,
    )
    op.create_index(
        "uq_fundamental_ticker_date",
        "fundamental_data",
        ["ticker_id", "retrieved_at"],
        unique=True,
    )
    op.create_table(
        "news_articles",
        sa.Column("article_id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("ticker_id", sa.Integer(), nullable=False),
        sa.Column("title", sa.Text(), nullable=False),
        sa.Column("url", sa.Text(), nullable=False),
        sa.Column("publisher", sa.String(length=100), nullable=True),
        sa.Column("published_at", sa.DateTime(timezone=True), nullable=False),
        sa.Column("retrieved_at", sa.DateTime(timezone=True), nullable=False),
        sa.ForeignKeyConstraint(
            ["ticker_id"], ["tickers.ticker_id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("article_id"),
    )
    op.create_index(
        "idx_news_ticker_published",
        "news_articles",
        ["ticker_id", "published_at"],
        unique=False,
        postgresql_using="btree",
        postgresql_ops={"published_at": "DESC"},
    )
    op.create_index(
        op.f("ix_news_articles_published_at"),
        "news_articles",
        ["published_at"],
        unique=False,
    )
    op.create_index(
        "uq_news_ticker_url", "news_articles", ["ticker_id", "url"], unique=True
    )
    op.create_table(
        "watchlist_tickers",
        sa.Column("watchlist_id", sa.Integer(), nullable=False),
        sa.Column("ticker_id", sa.Integer(), nullable=False),
        sa.Column(
            "added_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.ForeignKeyConstraint(
            ["ticker_id"], ["tickers.ticker_id"], ondelete="CASCADE"
        ),
        sa.ForeignKeyConstraint(
            ["watchlist_id"], ["watchlists.watchlist_id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("watchlist_id", "ticker_id"),
    )
    op.create_index(
        "idx_watchlist_tickers_ticker", "watchlist_tickers", ["ticker_id"], unique=False
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index("idx_watchlist_tickers_ticker", table_name="watchlist_tickers")
    op.drop_table("watchlist_tickers")
    op.drop_index("uq_news_ticker_url", table_name="news_articles")
    op.drop_index(op.f("ix_news_articles_published_at"), table_name="news_articles")
    op.drop_index(
        "idx_news_ticker_published",
        table_name="news_articles",
        postgresql_using="btree",
        postgresql_ops={"published_at": "DESC"},
    )
    op.drop_table("news_articles")
    op.drop_index("uq_fundamental_ticker_date", table_name="fundamental_data")
    op.drop_index(
        op.f("ix_fundamental_data_retrieved_at"), table_name="fundamental_data"
    )
    op.drop_index(
        "idx_fundamental_ticker_date",
        table_name="fundamental_data",
        postgresql_using="btree",
        postgresql_ops={"retrieved_at": "DESC"},
    )
    op.drop_table("fundamental_data")
    op.drop_index("uq_statement_ticker_line_item", table_name="financial_statements")
    op.drop_index(
        "idx_statement_ticker_type_quarter",
        table_name="financial_statements",
        postgresql_using="btree",
        postgresql_ops={"fiscal_year": "DESC"},
    )
    op.drop_index("idx_statement_line_item", table_name="financial_statements")
    op.drop_table("financial_statements")
    op.drop_index("uq_earnings_ticker_period", table_name="earnings_data")
    op.drop_index(
        "idx_earnings_ticker_quarter",
        table_name="earnings_data",
        postgresql_using="btree",
        postgresql_ops={"fiscal_year": "DESC"},
    )
    op.drop_table("earnings_data")
    op.drop_index(
        "idx_daily_prices_ticker_date",
        table_name="daily_prices",
        postgresql_using="btree",
    )
    op.drop_index("idx_daily_prices_date", table_name="daily_prices")
    op.drop_table("daily_prices")
    op.drop_index(
        "idx_collection_schedules_next_run",
        table_name="collection_schedules",
        postgresql_where=sa.text("is_enabled = true"),
    )
    op.drop_table("collection_schedules")
    op.drop_index(op.f("ix_collection_jobs_ticker_id"), table_name="collection_jobs")
    op.drop_index(op.f("ix_collection_jobs_status"), table_name="collection_jobs")
    op.drop_index("idx_collection_jobs_started", table_name="collection_jobs")
    op.drop_table("collection_jobs")
    op.drop_index(
        "uq_rating_ticker_date_firm",
        table_name="analyst_ratings",
        postgresql_where="firm IS NOT NULL",
    )
    op.drop_index(op.f("ix_analyst_ratings_rating_date"), table_name="analyst_ratings")
    op.drop_index(
        "idx_rating_ticker_date",
        table_name="analyst_ratings",
        postgresql_using="btree",
        postgresql_ops={"rating_date": "DESC"},
    )
    op.drop_table("analyst_ratings")
    op.drop_table("watchlists")
    op.drop_index(op.f("ix_tickers_symbol"), table_name="tickers")
    op.drop_index(op.f("ix_tickers_sector"), table_name="tickers")
    op.drop_index(op.f("ix_tickers_industry"), table_name="tickers")
    op.drop_table("tickers")
    # ### end Alembic commands ###
